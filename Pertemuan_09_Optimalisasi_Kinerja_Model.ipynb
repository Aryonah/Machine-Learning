{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHk6x_NKasy4"
      },
      "source": [
        "# Pertemuan 09 - Optimalisasi Kinerja Model Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTMOqmjcasy5"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## 🔧 1. Definisi, Dampak, dan Pentingnya Optimalisasi Model\n",
        "\n",
        "### 📌 Definisi\n",
        "Optimalisasi model adalah proses penyesuaian parameter & hyperparameter untuk memaksimalkan kinerja model machine learning dalam hal akurasi, efisiensi, dan generalisasi.\n",
        "\n",
        "### 💡 Dampak Positif Optimalisasi Model\n",
        "✅ Akurasi meningkat: Model jadi lebih presisi dalam klasifikasi/prediksi.\n",
        "\n",
        "✅ Kesalahan berkurang: Mengurangi false positive/negative.\n",
        "\n",
        "✅ Efisiensi komputasi: Waktu training/inference lebih cepat.\n",
        "\n",
        "### 🔥 Kenapa Optimalisasi Itu Penting?\n",
        "🛠️ Menjamin keandalan model dalam menghadapi data baru.\n",
        "\n",
        "🌍 Membantu model bertahan di kondisi data dunia nyata (misalnya imbalanced dataset, noise).\n",
        "\n",
        "🧠 Mendukung pengambilan keputusan berbasis data yang lebih baik.\n",
        "\n",
        "### 🧪 2. Hyperparameter Tuning\n",
        "Hyperparameter adalah konfigurasi yang ditentukan sebelum training (contoh: n_estimators, max_depth, learning_rate).\n",
        "\n",
        "⚙️ Kenapa perlu dituning?\n",
        "🔺 Akurasi meningkat\n",
        "\n",
        "🧠 Menghindari overfitting & underfitting\n",
        "\n",
        "⚡ Efisiensi komputasi meningkat\n",
        "\n",
        "### 📊 3. Grid Search\n",
        "Mencoba setiap kombinasi hyperparameter dari grid yang didefinisikan.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 200}\n",
            "Best Score: 0.9583333333333334\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# 1. Load data\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Bagi data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Definisikan model dan parameter grid\n",
        "model = RandomForestClassifier()\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30]\n",
        "}\n",
        "\n",
        "# 4. Lakukan Grid Search\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 5. Cetak hasil terbaik\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ Kelebihan: Menjamin hasil optimal dalam ruang pencarian\n",
        "\n",
        "❌ Kekurangan: Lambat jika kombinasi terlalu banyak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🎲 4. Random Search\n",
        "\n",
        "Mencoba kombinasi secara acak dari distribusi yang ditentukan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'n_estimators': 100, 'max_depth': None, 'bootstrap': True}\n",
            "Best Score: 0.95\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy')\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Score:\", random_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ Lebih efisien waktu dibanding grid search\n",
        "\n",
        "❌ Tidak selalu menemukan kombinasi paling optimal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🧠 5. Bayesian Optimization\n",
        "Menggunakan pendekatan probabilistik (misalnya Gaussian Process) untuk memilih hyperparameter terbaik berdasarkan histori hasil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: OrderedDict({'bootstrap': False, 'max_depth': 16, 'n_estimators': 75})\n",
            "Best Score: 0.95\n"
          ]
        }
      ],
      "source": [
        "from skopt import BayesSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "param_dist = {\n",
        "    'n_estimators': (50, 200),\n",
        "    'max_depth': (10, 30),\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "bayes_search = BayesSearchCV(model, search_spaces=param_dist, n_iter=10, cv=5, scoring='accuracy')\n",
        "bayes_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", bayes_search.best_params_)\n",
        "print(\"Best Score:\", bayes_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "✅ Lebih efisien dan cerdas dibanding grid/random\n",
        "\n",
        "❌ Butuh pemahaman statistik yang lebih dalam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🧹 6. Feature Selection\n",
        "Memilih fitur yang paling relevan agar model lebih sederhana dan akurat.\n",
        "\n",
        "#### 🎯 Tujuan:\n",
        "Meningkatkan akurasi\n",
        "\n",
        "Mengurangi overfitting\n",
        "\n",
        "Mempercepat training\n",
        "\n",
        "Meningkatkan interpretabilitas\n",
        "\n",
        "#### 📊 a. Filter Methods\n",
        "Memilih fitur berdasar nilai statistik sebelum pelatihan model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: ['petal length (cm)', 'petal width (cm)']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "selector = SelectKBest(score_func=chi2, k=2)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "\n",
        "print(\"Selected features:\", X.columns[selector.get_support()].tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Contoh metrik:\n",
        "\n",
        "Chi-Square\n",
        "\n",
        "Correlation\n",
        "\n",
        "Mutual Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🔁 b. Wrapper Methods\n",
        "Menggunakan model untuk mengukur kinerja subset fitur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False  True  True]\n",
            "Feature ranking: [3 2 1 1]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "rfe = RFE(model, n_features_to_select=2)\n",
        "fit = rfe.fit(X, y)\n",
        "\n",
        "print(\"Selected features:\", fit.support_)\n",
        "print(\"Feature ranking:\", fit.ranking_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metode umum:\n",
        "\n",
        "Recursive Feature Elimination (RFE)\n",
        "\n",
        "Forward Selection\n",
        "\n",
        "Backward Elimination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🧩 c. Embedded Methods\n",
        "Seleksi fitur langsung saat pelatihan model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: ['sepal length (cm)', 'petal length (cm)', 'petal width (cm)']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "lasso = Lasso(alpha=0.01)\n",
        "lasso.fit(X, y)\n",
        "\n",
        "selected_features = X.columns[(lasso.coef_ != 0)]\n",
        "print(\"Selected features:\", selected_features.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Contoh:\n",
        "\n",
        "Lasso (L1 Regularization)\n",
        "\n",
        "Decision Tree Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### c. Embedded Methods (lanjutan)\n",
        "Contoh Implementasi (Lasso Regression):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features by Lasso: ['petal length (cm)']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Lasso Regression\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X, y)\n",
        "\n",
        "# Menampilkan fitur yang dipilih (koefisien tidak nol)\n",
        "selected_features = X.columns[lasso.coef_ != 0].tolist()\n",
        "print(\"Selected features by Lasso:\", selected_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Contoh Implementasi (Decision Tree Feature Importance):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features by Decision Tree: ['sepal length (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "Feature importances:\n",
            " petal width (cm)     0.448792\n",
            "petal length (cm)    0.420400\n",
            "sepal length (cm)    0.101328\n",
            "sepal width (cm)     0.029481\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Fit model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Menampilkan pentingnya fitur\n",
        "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "selected_features = feature_importances[feature_importances > 0.1].index.tolist()\n",
        "\n",
        "print(\"Selected features by Decision Tree:\", selected_features)\n",
        "print(\"Feature importances:\\n\", feature_importances.sort_values(ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Feature Extraction\n",
        "Feature extraction adalah proses mengubah data asli menjadi representasi fitur baru yang lebih relevan dan informatif. Berbeda dengan feature selection yang memilih subset dari fitur yang ada, feature extraction menciptakan fitur baru berdasarkan data yang ada.\n",
        "\n",
        "### Tujuan:\n",
        "Mengungkap struktur tersembunyi dari data\n",
        "\n",
        "Mengurangi dimensi data (dengan mempertahankan informasi penting)\n",
        "\n",
        "Meningkatkan kinerja model\n",
        "\n",
        "### Teknik Populer:\n",
        "Principal Component Analysis (PCA)\n",
        "\n",
        "Linear Discriminant Analysis (LDA)\n",
        "\n",
        "t-SNE (untuk visualisasi)\n",
        "\n",
        "Autoencoder (dalam deep learning)\n",
        "\n",
        "### Contoh Implementasi (PCA):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Explained Variance Ratio: [0.72962445 0.22850762]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standarisasi data\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# PCA untuk 2 komponen utama\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Polynomial Features\n",
        "Polynomial Features adalah metode feature engineering yang membuat fitur baru berdasarkan kombinasi polinomial dari fitur yang ada. Ini berguna untuk model linier agar dapat menangkap hubungan non-linier dalam data.\n",
        "\n",
        "### Contoh:\n",
        "Jika kita memiliki fitur x, maka polynomial degree 2 akan menghasilkan:\n",
        "\n",
        "- 1 (bias)\n",
        "\n",
        "- x\n",
        "\n",
        "- x²\n",
        "\n",
        "### Contoh Implementasi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original number of features: 4\n",
            "Number of features after Polynomial: 14\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Polynomial Features degree 2\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "print(\"Original number of features:\", X.shape[1])\n",
        "print(\"Number of features after Polynomial:\", X_poly.shape[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Penutup\n",
        "Optimalisasi model adalah kunci dalam membuat model machine learning yang akurat, efisien, dan handal. Dengan kombinasi dari hyperparameter tuning, feature selection, dan feature engineering seperti polynomial features, kita bisa meningkatkan performa prediksi sekaligus menjaga efisiensi komputasi."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
